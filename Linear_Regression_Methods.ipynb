{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab99e270",
   "metadata": {},
   "source": [
    "# Programming Assignment 1: Linear Regression\n",
    "\n",
    "## Part I: Introduction of Various Linear Regression Fitting Methods\n",
    "\n",
    "### Import the packages\n",
    "\n",
    "In class, we have learned linear regression. In this section, we will take through a simple experiment to see how it works. We are going to use `numpy` , `pandas`, `matplotlib` and `tensorboardX` packages in Python, the first thing we need to do is to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f94c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "# time packages are used to measure the convergence speed\n",
    "import time\n",
    "# there might be some warnings due to the different versions of python and packages you installed.\n",
    "# here we choose to suppress these warnings.\n",
    "# but don't ignore warnings unless you know you are absolutely right!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "num_features = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1871f",
   "metadata": {},
   "source": [
    "## I. Linear Regression Using Normal Equation\n",
    "\n",
    "Our experiment consists of three steps:\n",
    "1. Generation of training dataset.\n",
    "2. Implementation of our models\n",
    "3. Visualization of experimental results.\n",
    "\n",
    "And we will have an (analytical) open question in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a99018",
   "metadata": {},
   "source": [
    "### Generation of training dataset\n",
    "Linear model is defined as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{y}=\\boldsymbol {X \\boldsymbol\\theta}+ \\boldsymbol \\varepsilon,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol X = [\\boldsymbol x^{(1)}\\cdots \\boldsymbol x^{(m)}]^T$ is the observation matrix which consists of $m$ samples, $\\boldsymbol x^{(i)}$ is the feature vector of $i$-th sample, whose length is also known as *the number of features*; $\\boldsymbol \\theta$ is the weight vector, and $\\boldsymbol\\varepsilon$ is the Gaussian noise. We will generate training data based on the defination of linear model. Here the number of features is set to $2$. For simplicity, we just set $\\boldsymbol \\theta = \\boldsymbol 1$, where $\\boldsymbol 1$ is a vector with all components equal to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9274851",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 2\n",
    "theta_ = np.array([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f72d90",
   "metadata": {},
   "source": [
    "Given the the number of samples, we generate a random observation matrix $\\boldsymbol X$. The corresponding labels $\\boldsymbol y$ are computed through $\\boldsymbol{X\\theta}$ plus Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8d8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "\n",
    "def generate_data(num_samples, num_features, scale, function):\n",
    "    # range of x: [-0.5, 0.5]\n",
    "    X = np.random.rand(num_samples, num_features)-0.5\n",
    "    # X@y is equivalent to np.matmul(X, y)\n",
    "    y = function(X) + np.random.normal(scale=scale, size=(num_samples))\n",
    "    return X, y\n",
    "\n",
    "def func1(X):\n",
    "    return X @ theta_\n",
    "\n",
    "X, y = generate_data(num_samples, num_features, 0.2, func1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ef4ef",
   "metadata": {},
   "source": [
    "### Implementation of our models\n",
    "In real-world problems, we can only get $\\boldsymbol {X}$ and $\\boldsymbol {y}$. $\\boldsymbol\\theta$ is the weight vector we want to esimate through linear regression. To minimize the noise effect, we do a minimization problem\n",
    "$$\n",
    "\\min _{\\boldsymbol\\theta}\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}\n",
    "$$\n",
    "and we can get the best estimator by solving the normal equation\n",
    "$$\n",
    "\\boldsymbol X^{T} \\boldsymbol {X\\theta}=\\boldsymbol{X}^T \\boldsymbol{y},\\\\\n",
    "\\boldsymbol \\theta =  (\\boldsymbol X^{T} \\boldsymbol X^T)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}.\n",
    "$$\n",
    "Transfer the above formula into code and we get the analytical solution of linear regression. In the review section, we have learned that the inversion of matrix can be computed through `numpy.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y):\n",
    "    return np.linalg.inv((X.transpose()@X))@X.transpose()@y\n",
    "\n",
    "theta_linear_regression = linear_regression(X, y)\n",
    "print('the estimated parameter using normal equation is {}'.format(theta_linear_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3292fb",
   "metadata": {},
   "source": [
    "We have finished estimating the parameter of a linear regression model by normal equation, and the result turns out to be well fitted.\n",
    "\n",
    "However, we also learned another methood, Gradient Descent, to update the parameter till optimal. Let's implement it and compare the result with the one using analytical method.\n",
    "\n",
    "## II. Linear Regression Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b555645",
   "metadata": {},
   "source": [
    "### Implementation of our models\n",
    "In this method, we want to use gradient descent to find the optimal $\\theta$ to minimize the least square error\n",
    "$$\n",
    "\\begin{aligned}\n",
    "min J(\\theta) = \n",
    "& \\min _{\\boldsymbol\\theta}\\frac{1}{2}\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2} \\\\\n",
    "= & \\min _{\\boldsymbol\\theta}\\frac{1}{2} (X\\theta-Y)^T(X\\theta-Y).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Rewriting the error into Matrix format to process with batches of data will greatly accelerate the running. \n",
    "\n",
    "From the review seesion of the calculus, we have learned that the gradient of the aforementioned expression of theta is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\boldsymbol\\theta) = & \\frac{\\partial \\frac{1}{2} (X\\theta-Y)^T(X\\theta-Y)}{\\partial \\theta} \\\\\n",
    "= & \\boldsymbol X^TX\\theta - X^TY\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we can use this gradient to iteratively update the parameter through\n",
    "$$\n",
    "\\boldsymbol\\theta := \\boldsymbol\\theta - \\alpha \\nabla J(\\boldsymbol\\theta),\n",
    "$$\n",
    "where $\\alpha$ is the learning rate. Since we want to maximize $\\mathcal l(\\boldsymbol \\theta)$, the update rule becomes\n",
    "$$\n",
    "\\boldsymbol \\theta:=\\boldsymbol \\theta+ \\alpha (\\boldsymbol X^TX\\theta - X^TY).\n",
    "$$\n",
    "Now we transfer this formula into codes. Instead of using a for-loop, we prefer to use matrix operations as it's much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea44aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we initialize the theta\n",
    "theta_linear_regression_2 = np.zeros(num_features)\n",
    "theta_linear_regression_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdb807",
   "metadata": {},
   "source": [
    "Now, we implement the batch gradient descent algorithm for linear regression as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c45061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set up some coefficiency of our linear regression model\n",
    "max_epoch = 5000\n",
    "alpha = 0.01\n",
    "\n",
    "# initialize the writer\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "# function to compute the error\n",
    "def LSError(X, y, theta):\n",
    "    return np.linalg.norm(np.matmul(X, theta)-y)\n",
    "\n",
    "# implementation of batch gradient descent\n",
    "def BGD(X, y, max_epoch, alpha, theta_linear_regression):\n",
    "    for epoch in range(max_epoch):\n",
    "        # if the least square error is small enough, we can stop the iterative update\n",
    "        err = LSError(X, y, theta_linear_regression)\n",
    "        # record the loss in each epoch\n",
    "        writer.add_scalar('BGD_LSError', err, epoch)\n",
    "        if err <= 1e-5:\n",
    "            break\n",
    "        # each epoch we use whole data to compute the gradient and update the parameter\n",
    "        deriv = np.matmul(np.matmul(X.T, X), theta_linear_regression_2) - np.matmul(X.T, y)\n",
    "        # update the parameter\n",
    "        theta_linear_regression -= alpha* deriv\n",
    "    return theta_linear_regression\n",
    "\n",
    "BGD(X, y, max_epoch, alpha, theta_linear_regression_2)\n",
    "theta_linear_regression_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf5972",
   "metadata": {},
   "source": [
    "This algorithm is known as Batch Gradient Descent (BGD) because it only updates the weights after it have seen all training samples. \n",
    "Compared to normal equation, the result seems both coverges to the ground truth.\n",
    "\n",
    "But if we have a large volume of data, the batch gradient descent will be time-consuming. Therefore, we try to use Stocastic Gradient Descent.\n",
    "\n",
    "## III. Linear Regression with Stochastic Gradient Descent\n",
    "\n",
    "In the class we also learned Stochastic Gradient Descent (SGD), in which we only use one sample to compute the gradient and update the weight:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{for } i=&1\\cdots m,\\\\\n",
    "&\\boldsymbol \\theta_{j}:=\\boldsymbol \\theta_{j}+\\alpha\\left(y^{(i)}-h_{\\boldsymbol \\theta}\\left(\\boldsymbol x^{(i)}\\right)\\right) \\boldsymbol x_{j}^{(i)} \\text { for every } j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "We know that SGD has a much faster convergence speed than BGD especially on large dataset. Now let's do some experiments to verify it. First, we need to generate a larger dataset.\n",
    "\n",
    "#### Q1: Please implement linear regression using SGD by filling in the blanks. (Imitate the format of the BGD and name your estimated parameter as theta_linear_regression_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942aaea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please implement SGD in this cell\n",
    "# code in function format imitating the previous BGD\n",
    "# parameters: inpuy samples X, corresponding labels y, maximum number of iteration max_epoch, learning rate alpha, initialized theta theta_linear_regression3\n",
    "# return: theta_linear_regression3\n",
    "\n",
    "'''\n",
    "\n",
    "         your code\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774cb64",
   "metadata": {},
   "source": [
    "### Analytical Question (Optional)\n",
    "#### Optional Q1: Compare the result(should be included in your answer) between BGD and SGD and analyze the similarity and difference. *Hint: you can use tensorboard to visualize loss curves with:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir [your_log_dir]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283be8a4",
   "metadata": {},
   "source": [
    "\n",
    "#### *and compare the similarity and difference of the learning curves.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af999d8",
   "metadata": {},
   "source": [
    "**Your answer:** *Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa1ceb",
   "metadata": {},
   "source": [
    "## IV. Ridge Regression Using Normal Equation\n",
    "### Implementation of Our Models\n",
    "By now, we have implemented our linear regression model. We can also add a regularization in linear model and we have\n",
    "$$\n",
    "\\min _{\\boldsymbol\\theta}\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}+\\alpha\\|\\boldsymbol{\\theta}\\|^{2},\n",
    "$$\n",
    "where $\\alpha \\geq 0$ is a hyper-parameter and defines the strength of regularization. This method is known as *ridge regression*. Minimizing the loss $\\|\\boldsymbol y-\\boldsymbol X \\boldsymbol{\\theta}\\|^{2}+\\alpha\\|\\boldsymbol{\\theta}\\|^{2}$ is equivalent to solve the normal equation \n",
    "$$\n",
    "\\left(\\boldsymbol X^{T} \\boldsymbol X+\\alpha \\boldsymbol I\\right) \\boldsymbol{\\theta}=\\boldsymbol X^{T} \\boldsymbol{y},\\\\\n",
    "\\boldsymbol{\\theta} = \\left(\\boldsymbol X^{T} \\boldsymbol X+\\alpha \\boldsymbol I\\right)^{-1}\\boldsymbol X^{T} \\boldsymbol{y}.\n",
    "$$\n",
    "#### Q2: Please implement ridge regression by completing following code. You should give an analytical solution instead of using gradient descent to get a numerical solution. (imitate the format of the BGD and name your estimated parameter as theta_ridge_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please implement your code in this cell\n",
    "def ridge_regression(X, y, alpha = 1):\n",
    "    # parameters: inpuy samples X, corresponding labels y, regularizer weight alpha (1 in default)\n",
    "    # return: estimated theta\n",
    "    # replace following blanks with your implementation\n",
    "    '''\n",
    "\n",
    "             your code\n",
    "\n",
    "\n",
    "    '''\n",
    "        \n",
    "# alpha=1 in default but it can change for the optimization of the final result\n",
    "# for alpha in np.arange(0, 0.1, 0.01):\n",
    "theta_ridge_regression = ridge_regression(X, y)\n",
    "print('the estimated parameter using normal equation is {}'.format(theta_ridge_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe5fc5",
   "metadata": {},
   "source": [
    "### Visualization of results\n",
    "So far we have finished the implementation of linear regression and ridge regression models. To better understand our work, we will plot train data, linear regression model and ridge regression model on the same figure using `matplotlib` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "x_axis = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 20)\n",
    "y_axis = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 20)\n",
    "x_axis, y_axis = np.meshgrid(x_axis, y_axis)\n",
    "z_axis_1 = theta_linear_regression[0] * x_axis + theta_linear_regression[1] * y_axis\n",
    "z_axis_2 = theta_ridge_regression[0] * x_axis + theta_ridge_regression[1] * y_axis\n",
    "# alpha means the transparency of image\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_1, alpha = 0.6)\n",
    "ax.plot_surface(x_axis, y_axis, z_axis_2, alpha = 0.6)\n",
    "# how the first two legend corrspond to its surface correctly?\n",
    "fake_sct_1 = ax.scatter([], [], [], label=\"linear regression\")\n",
    "fake_sct_2 = ax.scatter([], [], [], label=\"ridge regression\")\n",
    "sct = ax.scatter(X[:, 0], X[:, 1], y, label=\"train data\", color='red')\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cc972",
   "metadata": {},
   "source": [
    "## V. Locally Weighted Linear Regression\n",
    "\n",
    "Sometimes, the samples we encountered may not be linearly distributed. We can fit a higher order model like quadratic model, but it is hard to predetermine the feature we extract from the data, which may cause underfitting or overfitting. Therefore, locally weighted linear regression, a non-parametric model, can be used to fit the samples. The parameter $\\omega$ depends on the particular point x at which we are trying to evaluate.\n",
    "\n",
    "+ Fit $\\theta$ to minimize \n",
    "$\n",
    "\\sum _{\\boldsymbol i}\\omega^{(i)}(\\boldsymbol y^{(i)}-\\boldsymbol{\\theta^{(i)}} \\boldsymbol x^{(i)} )^{2},\n",
    "$\n",
    "+ output $\\theta^T x$\n",
    "\n",
    "The $\\omega^{(i)}$ is the non-negetive valued weightes. Obvously, higher $\\omega^{(i)}$ will attach greater importance to its corresponding error $(\\boldsymbol y^{(i)}-\\boldsymbol{\\theta^{(i)}} \\boldsymbol x^{(i)} )^{2}$, while ignoring those terms with lower $\\omega^{(i)}$.\n",
    "\n",
    "Usually we choose to compute the $\\omega^{(i)}$ as, \n",
    "\n",
    "$$\n",
    "\\omega^{(i)} = exp\\Big( -\\frac{(x^{(i)} − x)^2}{2\\tau^2}\\Big)\n",
    "$$\n",
    "\n",
    " Moreover, if $|x^{(i)} − x|$ is small, then $\\omega^{(i)}$ is close to 1; and if $|x^{(i)} − x|$ is large, then $\\omega^{(i)}$ is small.\n",
    " \n",
    " The $\\tau$, the bandwidth parameter, controls how quick a training example $x^{(i)}$ deviates from the query point x.\n",
    "\n",
    "\n",
    "\n",
    "### Synthsize the dataset\n",
    "\n",
    "We need to generate a dataset with non-linearity. Here we can use $\\sin$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52395a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_2 = 1\n",
    "num_samples_2 = 300\n",
    "\n",
    "def generate_data2(num_samples, num_features,  function):\n",
    "    # range of x\n",
    "    X = np.linspace(-6, 6, 300).reshape(-1, 1)\n",
    "    # X@y is equivalent to np.matmul(X, y)\n",
    "    y = function(X)\n",
    "    return X, y\n",
    "\n",
    "def mysin(X):\n",
    "    return np.sin(X).reshape(num_samples_2,)\n",
    "\n",
    "X_2, y_2 = generate_data2(num_samples_2, num_features_2, mysin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff67894-bac2-4b7c-9cd4-014d64369f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2, y_2)\n",
    "plt.title('samples frpm sin(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13d87ca",
   "metadata": {},
   "source": [
    "At the above code, we have generated 100 samples from $\\sin$ function. In the following step, we need to fit an linear regression model by using locally weighted linear regression method.\n",
    "\n",
    "#### Q3: Please implement locally weighted linear regression to fit a model which best describes the distribution of samples we generated previously. Please try different hyper-parameter bandwidth $\\tau$ to make your model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aa04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace following blanks with your implementation\n",
    "def locally_weighted_Linear_Regression(X, y, max_epoch, alpha, tau, win_len):\n",
    "    # parameters: inpuy samples X, corresponding labels y, , maximum number of iteration max_epoch, learning rate alpha, bandwidth tau, slide window lwngth win_len\n",
    "    # return: The MSEerror between estimated label for each x(i) and ground truth label\n",
    "    # MSEerror mwans the average of the summation of the squared error between the prediction and ground truth, which is a metracs to measure the fitness of a model\n",
    "    # hint: for each linear model we only have 10 samples, please set your lr a little bit higher for better performance\n",
    "    '''\n",
    "\n",
    "             your code\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "MSE_error_LWLR = locally_weighted_Linear_Regression(X_2, y_2, 500, 0.0001, 1, 6)\n",
    "print('the MSE Loss of the Locally weighted Linear Regression is {}'.format(MSE_error_LWLR))\n",
    "\n",
    "# compute the MESerror of the basic linear model using Normal euqation\n",
    "theta_LR = linear_regression(X_2, y_2)\n",
    "y_pred_LR = X_2 @ theta_LR\n",
    "MSE_error_LR = np.linalg.norm(y_pred_LR - y_2)/num_samples_2\n",
    "print('the MSE Loss of the Linear Regression is {}'.format(MSE_error_LR))\n",
    "plt.scatter(X_2, y_pred_LR, color='g')\n",
    "plt.plot(X_2, y_2, color='black', linewidth=2.5)\n",
    "plt.legend(['Locally weighted LR', 'Linear Regression', 'graoundtruth'])\n",
    "plt.title('Fitted curve of the sin(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9de11",
   "metadata": {},
   "source": [
    "### Analytic Question\n",
    "\n",
    "#### Q4: Please plot your result and discuss the effect of $\\tau$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594b4e1",
   "metadata": {},
   "source": [
    "**Your answer:** *Write your answer here.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
